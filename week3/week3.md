# Gradient Descent Algorithms
## 여러가지 Gradient Descent 알고리즘들:
* Batch Gradient Descent
* Mini-batch Gradient Descent
* Stochastic Gradient Descent
* Momentum
* Nesterov Accelerated Gradient (NAG)
* Adagrad
* RMSprop
* AdaDelta
* Adam

## 링크
* 한국서 요약 정리 설명 [링크](http://shuuki4.github.io/deep%20learning/2016/05/20/Gradient-Descent-Algorithm-Overview.html)

* 영문 버젼 설명 [링크](http://ruder.io/optimizing-gradient-descent/)

* 코딩 도움 [링크](https://wiseodd.github.io/techblog/2016/06/22/nn-optimization/)

## 코딩:
Stochastic Gradient Descents 까지만 해보기
